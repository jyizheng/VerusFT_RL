# Verus Dataset Collection & Evaluation ‚Äî Progress Update

This repository collects datasets and evaluation infrastructure for post-training and evaluating large language models on Verus-based deductive program verification. The goal is to support scalable, realistic training and evaluation of models that generate sound and complete specifications and proofs for Rust programs verified with Verus.

---

## Overview

We collect diverse Verus-related supervision signals and use them to:

- Post-train code models for Verus specification and proof generation.
- Evaluate generalization under realistic, repository-level holdout settings.
- Measure specification soundness and completeness via executable proxies.

The repository is organized to cleanly separate training data, held-out evaluation data, and evaluation pipelines.

---

## Dataset Sources

We currently construct samples from four complementary sources:

1. **Coq ‚Üí Verus Translation**
   - Translated examples from existing Coq datasets.
   - Provides mathematically grounded specifications.
   - Useful for learning logical structure and invariant patterns.

2. **`verus_syn` from Verus Source Files**
   - Automatically synthesized samples derived from Verus code and resolved dependencies.

3. **Minimizer Snapshots**
   - Collected from Verus standalone benchmarks.

4. **AI Coding Agent Outputs**
   - Generated by an AI coding agent operating on Verus project files.
   - Reflects realistic agent-style generation errors and successes.

---

## Training Plan

We will post-train large code models starting with:

- Qwen3-Coder-70B
- Qwen3-Coder-480B

Before any post-training, we evaluate baseline performance on both open-source and closed models (where available). This ensures improvements can be attributed to data and training rather than evaluation artifacts.

---

## Evaluation Strategy

### Repository-Level Holdout

To mimic real-world deployment scenarios:

- One entire source repository is held out from training.
- This repository is used exclusively for evaluation.
- The holdout prevents leakage across closely related files and specs and better reflects how models are used on unseen codebases.

### Specification Evaluation

We evaluate generated specifications using an executable proxy:

1. Generate positive and negative input/output pairs from code.
2. Check whether the generated `requires` / `ensures` formulas:
   - Accept valid behaviors (soundness).
   - Reject invalid behaviors (completeness).
3. Aggregate results into quantitative metrics.

This enables scalable evaluation without requiring full manual proof inspection.

---

## Progress Update

- **Data collection**: Pipelines for Coq translations, `verus_syn` synthesis, minimizer snapshots, and agent-generated samples are in place and producing tagged examples.
- **Baseline evaluation**: Pre-training baselines are being recorded for both open and closed models to isolate post-training gains.
- **Spec evaluation harness**: Executable proxy checks for `requires`/`ensures` coverage are scoped and being integrated into the evaluation pipelines.
- **Holdout protocol**: Repository-level holdout split defined to ensure realistic generalization tests.

---

## Contributing

Contributions are very welcome üôå

If you add data, scripts, or evaluation utilities:

- Follow the existing directory structure.
- Clearly document the data source and generation method.
- Submit a PR for review.

If you are unsure where something belongs, feel free to open a draft PR or discussion.

---

## Acknowledgements

Thanks to everyone who contributed to dataset collection, tooling, and design. This repository represents a significant collaborative effort, and we appreciate all the work that‚Äôs gone into it.

---

# VerusSFT: Fine-Tuning and RL for Verification-Oriented Rust/Verus Code

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

VerusSFT is a research repository for exploring **supervised fine-tuning (SFT)** and **reinforcement learning (RL)** of language models on **verification-oriented Rust/Verus code**. General-purpose code LLMs often struggle with Verus-specific concepts like `exec` / `ghost` / `proof` modes, `requires` / `ensures` specifications, View functions, typestate-like abstractions, loop invariants and `decreases` clauses, proof blocks, and Verus error traces. The goal is to make models Verus-capable and to understand when structured representations (like ASTs) add value beyond plain text.

## Table of Contents
- [Quick Start](#quick-start)
- [Project Status](#project-status)
- [Known Limitations](#known-limitations)
- [Project Goals](#project-goals)
- [Getting Started for Contributors](#getting-started-for-contributors)
- [Motivation](#motivation)
- [Related Work](#related-work)
- [Methodology](#methodology)
- [Reinforcement Learning Extensions](#reinforcement-learning-extensions-for-verification-agents)
- [Student Subprojects](#student-subprojects)
- [Installation](#installation)
- [Usage](#usage)
- [Configuration](#configuration)
- [Compute Requirements](#compute-requirements)
- [Dataset](#dataset)
- [File Structure](#file-structure)
- [FAQ](#faq)
- [Troubleshooting](#troubleshooting)
- [Future Improvements](#future-improvements)
- [Contributing](#contributing)
- [Citation](#citation)
- [License](#license)
- [Contact](#contact)

---

## Quick Start

```bash
# Install dependencies
pip install transformers trl datasets peft accelerate torch

# Train the model
python sft_example.py

# Test inference
python test_inference.py
```

---

## Project Status

**Current Implementation Status** (as of December 2025):

- ‚úÖ **Parameter-efficient fine-tuning with LoRA**
- ‚úÖ **10 diverse Verus training examples** (seed set, expanding soon)
- ‚úÖ **Configurable training hyperparameters**
- ‚úÖ **Inference script for testing trained models**
- ‚úÖ **Small adapter weights** (~6.2MB) instead of full model
- üöß **Minimizer integration** (planned)
- üìã **Multi-task dataset builder** (design phase)
- üìã **RL components** (design phase)

### Training Results (Prototype - Dec 2025)

| Metric | Initial | Final | Improvement |
|--------|---------|-------|-------------|
| Loss | 3.40 | 2.42 | 28.7% ‚Üì |
| Token Accuracy | 45% | 55% | 10% ‚Üë |
| Training Time | - | ~14s | 8 epochs |
| Adapter Size | - | 6.2MB | - |

> **Important**: These metrics are from prototype training on only 10 hand-crafted examples. They demonstrate the training pipeline works but do not represent real performance on verification tasks. Token accuracy is not a reliable proxy for verification success‚Äîthe true metric is whether generated code actually verifies with Verus.

---

## Known Limitations

**Current prototype limitations (as of Dec 2, 2025):**

- ‚ùå **Tiny dataset**: Only 10 training examples (not representative of real Verus code diversity)
- ‚ùå **No code-specialized base model**: Using GPT-2 instead of models trained on code
- ‚ùå **No Verus evaluation**: Training metrics (loss, token accuracy) don't measure actual verification success
- ‚ùå **No minimizer integration**: Dataset is hand-crafted, not automatically generated
- ‚ùå **Single-task only**: No multi-task training across spec generation, code synthesis, and repair
- ‚ùå **No benchmark suite**: No standardized held-out test set for evaluation

**What this means:**
- The prototype validates the *training infrastructure* (LoRA, SFT pipeline)
- It does NOT validate that fine-tuning improves Verus code generation
- Real evaluation requires running Verus on generated code and measuring verification pass rates

**Planned improvements:** See [Future Improvements](#future-improvements) and [Student Subprojects](#student-subprojects)

---

## Project Goals

### üéØ Primary Goals

| Goal | Description | Status |
|------|-------------|--------|
| **Dataset** | Build a minimized, high-quality text-only dataset of Verus code, specs, proofs, and error traces from open-source projects | üìã Planned |
| **Multi-Task SFT** | Train models on three core tasks (see below) | üöß In Progress |
| **Verus-Native Evaluation** | Evaluate using verification pass rate, not just syntax similarity | üìã Planned |

### Three Core Tasks

1. **Task A ‚Äî Code ‚Üí Specifications** ‚úÖ *Prototype Ready*
   - Input: Rust+Verus function body (no specs)
   - Output: `requires`/`ensures`, Views, invariants, `decreases` clauses

2. **Task B ‚Äî Specifications ‚Üí Verified Code** üöß *In Progress*
   - Input: Full specification + function signature
   - Output: Executable + ghost + proof code that verifies

3. **Task C ‚Äî Error-Guided Repair** üìã *Planned*
   - Input: Code + spec + Verus error message
   - Output: Patched invariant, spec, or proof block

### üî¨ Secondary Goal (Research Question)

**When do structured representations help?**
- After strong text-only baselines, introduce AST/structured encodings
- Measure incremental benefit via ablation studies
- Answer: *For what types of verification tasks do ASTs provide value beyond plain text?*

---

## Getting Started for Contributors

Welcome! This section helps you get started whether you're a student, researcher, or open-source contributor.

### For First-Time Contributors

**Quickstart path** (15 minutes):
1. Read [Motivation](#motivation) to understand the problem
2. Try the prototype: `python sft_example.py` (validates setup)
3. Review [Student Subprojects](#student-subprojects) to find a task
4. Check GitHub issues for `good-first-issue` labels

**What to read**:
- **Minimum**: [Project Goals](#project-goals), [Known Limitations](#known-limitations)
- **For dataset work**: [Methodology > Dataset Pipeline](#dataset-pipeline)
- **For training work**: [Configuration](#configuration), [Compute Requirements](#compute-requirements)
- **For evaluation**: [Phase 1: Evaluation](#phase-1-evaluation)

### For Researchers

**Deep dive path**:
1. Read the full [Methodology](#methodology) section
2. Study [Reinforcement Learning Extensions](#reinforcement-learning-extensions-for-verification-agents)
3. Review [Related Work](#related-work) and cited papers
4. Examine [Future Improvements](#future-improvements) for open problems

**Key research questions**:
- How much does Verus-specific fine-tuning improve over general code models?
- What verification tasks benefit most from multi-task training?
- When do structured representations (ASTs) provide value over text?
- How can offline RL leverage Verus minimizer outputs?

### For Students (Rotation/Undergrad Projects)

See [Student Subprojects](#student-subprojects) for scoped projects. Each includes:
- Difficulty level (Easy/Medium/Hard)
- Current status
- Clear deliverables

**Recommended progression**:
1. **Week 1-2**: Set up environment, run prototype, read methodology
2. **Week 3-4**: Choose subproject, implement minimal version
3. **Week 5-8**: Expand implementation, collect results
4. **Week 9-10**: Evaluation, documentation, presentation

### How to Contribute

1. **Start a discussion**: Open a GitHub issue describing your idea
2. **Get feedback**: Maintainers will help scope the work
3. **Make your changes**: Follow code style, add tests where applicable
4. **Submit PR**: Include clear description and link to related issue

**Areas needing help**:
- üìä Dataset generation scripts (Python)
- üß™ Evaluation harness for Verus verification
- üìö Documentation improvements
- üî¨ Experimental ablations and benchmarks

---

## Motivation

### The Problem

Formal verification in Verus embeds specifications, invariants, and proofs directly in Rust code. While systems like VeriStruct show that LLMs can assist with specification and proof tasks, **current general-purpose code models struggle** with:

| Challenge | Example Issue |
|-----------|---------------|
| **Verus Modes** | Confusing `exec`, `ghost`, and `proof` contexts |
| **Specifications** | Omitting or weakening `requires`/`ensures` clauses |
| **Proof Generation** | Generating syntactically valid but logically invalid proofs |
| **Error Interpretation** | Failing to understand Verus error messages and verification failures |
| **Ghost State** | Mishandling View functions and abstract specifications |

### Our Hypothesis

**Supervised fine-tuning on a curated corpus of minimized, self-contained Verus examples will significantly improve model performance**‚Äîeven *without* structured encodings.

**Key insights:**
- üì¶ **Minimized examples** are self-contained and focused
- üéØ **Multi-task training** covers the full verification workflow
- üî¨ **Text-first approach** establishes strong baselines before exploring structure
- üìä **Verus-native evaluation** measures actual verification success, not just syntax

**Research question:** Once we have strong text baselines, we can rigorously ask: *when do ASTs or other structural views actually help?*

---

## Related Work

VerusSFT builds on a growing body of literature exploring language-model-assisted verification:

- **[VeriStruct](https://github.com/ChuyueSun/VeriStruct)**: Focuses on prompt engineering and retrieval but stops short of supervised fine-tuning on minimized Verus corpora.
- **[arXiv:2505.20302](https://arxiv.org/pdf/2505.20302)**: Examines adjacent verification-aware fine-tuning strategies.
- **Key gaps this project addresses**:
  - Reproducible, minimized Verus datasets
  - Verus-native evaluation harnesses
  - Head-to-head comparisons between text-only and structure-augmented representations
  - Multi-task SFT with RL-based refinement

---

## Methodology

> **Note:** This section describes the complete research vision. The current prototype implements basic SFT training. The dataset pipeline, minimizer integration, and multi-task training are planned for future development.

The project is split into two main phases.

### Phase 1 ‚Äî Text-Only Dataset + SFT (Core)

We start from existing open-source Verus code, including:

- the main Verus repo (examples, tests, verified libraries),
- Verus-based projects listed on the [Verus publications/projects page](https://verus-lang.github.io/verus/publications-and-projects/), and
- internal benchmarks and artifacts (e.g., [VeriStruct](https://github.com/ChuyueSun/VeriStruct) data-structure modules).

Large modules and full repos are too big for individual SFT samples. Instead, we use the existing **Verus minimizer** to turn big programs into small, self-contained examples. When the minimizer is unavailable or a repo mixes plain Rust and Verus, follow the [Rust/Verus Code Extraction Method](./RUST_VERUS_EXTRACTION.md) to isolate dependency-free verifying snippets before serialization.

**Minimizer tool:**
- Location: [`source/tools/minimizers/`](https://github.com/ChuyueSun/verus/tree/main/source/tools/minimizers) in the Verus repo
- Purpose: given a target file or property, automatically shrink the program while preserving a chosen verification behavior (e.g., still verifies, or still fails with a particular error).

#### Dataset Pipeline

1. **Seed Collection**
   - Select diverse Verus files from examples, tests, and verified modules.

2. **Minimization**
   - Run the minimizer to get tiny verifying (and optionally failing) examples.
   - Each minimized file is a compact unit with exactly the code/specs/proofs needed for that behavior.

3. **Unit Extraction**
   From each minimized file, extract logical units:
   - Function-level units for **Task A (code ‚Üí spec)** and **Task B (spec ‚Üí code)**.
   - Lemma/proof-level units for **Task C (repair)**.
   - Module-level units only when necessary (e.g., shared View functions).

4. **JSONL Serialization (Text-Only)**
   Each dataset entry is stored as a text-only example, e.g.:

   ```json
   {
     "id": "...",
     "task": "spec_from_code",
     "input_text": "...",
     "target_text": "...",
     "metadata": {
       "repo": "...",
       "minimized": true,
       "verus_version": "...",
       "code_lines": 37
     }
   }
   ```

5. **Quality Filtering & Deduplication**
   - Remove trivial/uninteresting samples.
   - Deduplicate near-duplicates across repositories.

**Two complementary products from the minimizer.**
- **SFT dataset (Q/A style, no reasoning):** every minimized snippet is converted into question/answer pairs for Tasks A/B/C. The "question" is the task-specific prompt (e.g., "add the missing `requires`"), and the "answer" is the ground-truth spec or code. We deliberately keep the answers *reasoning-free* so that SFT teaches the model crisp completions without hallucinated thought chains.
- **RL trajectory dataset (reasoning-rich):** for the exact same minimized examples, we also log multi-step interaction traces (tool calls, intermediate hypotheses, Verus logs). These richer transcripts fuel offline RL algorithms by giving them access to explicit reasoning steps, not just final Q/A endpoints.

This yields datasets that are small, focused, and tailored separately for high-quality SFT and reasoning-aware RL.

### Phase 1: Text-Only SFT Tasks

We focus on three core tasks, all in plain Verus text (no AST yet):

#### Task A ‚Äî Code ‚Üí Specifications

Input: Rust+Verus function body (no specs or minimal specs).
Output:
- `requires` / `ensures`
- relevant Views / ghost arguments
- `invariant` / `decreases` clauses if loops are present

#### Task B ‚Äî Specifications ‚Üí Verified Code

Input: full specification (pre/postconditions, Views) and function signature.
Output: executable + ghost + proof code that verifies under Verus.

#### Task C ‚Äî Proof / Invariant Repair

Input: code + spec + Verus error message (or failing obligation info).
Output: patched invariant, spec, or proof block that fixes the failure.

Training is done in an instruction-style format compatible with downstream usage (e.g., [VeriStruct](https://github.com/ChuyueSun/VeriStruct) pipeline prompts).

### Phase 1: Evaluation

#### Primary Metric: Verification Pass Rate

**Definition**: Percentage of generated code that successfully verifies with Verus without errors.

**Measurement**:
- Run Verus on each generated example
- **Pass**: Code verifies with exit code 0
- **Fail**: Verification errors, timeouts (>30s), or syntax errors
- **Partial success**: Track individual specs/invariants that verify (for debugging)

#### Secondary Metrics

| Metric | Definition | Use Case |
|--------|------------|----------|
| **Syntax Correctness** | % that parse as valid Rust/Verus | Filter out broken generations |
| **Spec Completeness** | Are all necessary `requires`/`ensures` present? | Measure coverage |
| **Spec Tightness** | Are specs neither too weak nor too strong? | Measure quality |
| **Repair Size** | Lines changed when fixing errors (Task C) | Measure precision |
| **Error Type Breakdown** | % of each error category (see below) | Identify weaknesses |

#### Error Categories for Analysis

1. **Mode errors**: Incorrect `exec`/`ghost`/`proof` usage
2. **Missing preconditions**: Incomplete `requires` clauses
3. **Missing postconditions**: Incomplete `ensures` clauses
4. **Wrong invariants**: Loop invariants that don't hold
5. **Termination**: Missing or incorrect `decreases` clauses
6. **Type errors**: Rust type system violations
7. **VC failures**: Verification condition failures (proof obligations)
8. **Timeout**: Verification exceeds time limit

#### Benchmark Suite

**Planned test set** (to be assembled from minimized Verus code):

- **Basic operations**: 20 examples (arithmetic, comparisons, simple specs)
- **Data structures**: 30 examples (vectors, ring buffers, linked lists, trees, maps)
- **Concurrency**: 15 examples (atomics, locks, invariants)
- **Proof helpers**: 15 examples (lemmas, ghost code, View functions)
- **Real-world modules**: 20 examples (from verified projects)

**Total**: ~100 held-out examples across diverse categories

#### Baseline Comparisons

Compare fine-tuned models against:

1. **Zero-shot baselines**: GPT-4, Claude, Qwen2.5-Coder (no fine-tuning)
2. **Few-shot prompting**: With 3-5 in-context examples
3. **VeriStruct**: Retrieval-augmented prompting baseline
4. **Text-only SFT**: Our approach
5. **Structure-augmented SFT**: With AST/graph encodings (Phase 2)

---

## Reinforcement Learning Extensions for Verification Agents

While VerusSFT is anchored in supervised fine-tuning, it equally emphasizes reinforcement learning (RL) strategies for LLM-based verification agents. These extensions are directly motivated by systems like VeriStruct and DeepSeek-style reasoners, and they provide a roadmap for going beyond static prompting.

### 1. Three RL Paradigms

We distinguish three complementary paradigms for ‚Äúdoing RL on agents,‚Äù now annotated with how they map onto **online** (on-policy) versus **offline** (batch) reinforcement learning styles:

#### Paradigm 1: True policy-gradient RL on model weights (online RL)

- Treat the LLM as a parametric policy \(\pi_\theta(a \mid s)\).
- Interact with an environment built from Verus + benchmark suites.
- Define explicit rewards (verification success, proof quality, VC reductions).
- Update \(\theta\) with policy-gradient methods (PPO, GRPO, etc.).
- This is the paradigm used in RLHF and DeepSeek-style reasoning RL.

**Key property:** the model weights change over time; the policy improves in a lasting way.

#### Paradigm 2: Prompt-based / policy-shaped RL (online-style but weight-frozen)

- Freeze the base LLM and manipulate its ‚Äúpolicy‚Äù through prompt design, intermediate memories, and tool choices.
- Collect trajectories with success/failure signals (Verus errors, timeouts, successes).
- Perform reflection over these trajectories to write ‚Äúlessons learned,‚Äù update instructions, and curate successful exemplars.
- Future prompts include these reflections, distilled rules, and relevant snippets.

**Key property:** behavior improves via context, memory, and tool choices, not gradients.

#### Paradigm 3: Offline imitation + RL finetuning (offline-to-online hybrid)

- Stage 1: collect high-quality trajectories (VeriStruct data, expert-written proofs) and perform SFT to teach syntax/patterns.
- Stage 2: apply RL finetuning on top of SFT with rewards such as Verus success, compactness, or runtime to refine behavior.

**Key property:** combines the stability of imitation with the exploration of RL.

### 2. Implementation Sketches

#### 2.1 Paradigm 1 (True RL)

- Treat ‚ÄúVerus + benchmark suite‚Äù as the RL environment.
- **State:** task context, partially annotated code, verification logs.
- **Action:** LLM-generated edits or suggestions (spec changes, invariant tweaks, ghost code additions).
- **Transition:** apply edits, run Verus, observe new logs/errors.
- **Reward:** shaped by verification success, VC reductions, penalties for syntax errors or excessive code.
- **Loop:** sample proof attempts, evaluate rewards, update the model via PPO/GRPO, iterate.

This approach yields a verifier-specialized LLM that internalizes Verus syntax and proof idioms beyond SFT.

#### 2.2 Paradigm 2 (Prompt-based RL / Reflection)

- Keep the base LLM fixed but implement RL through tool-aware prompting.
- Define an agent loop that decides when to call Verus, edit code, query examples, or finalize proofs.
- Log trajectories (states, tool calls, edits, outcomes).
- Run a reflection prompt after each episode to extract lessons (error patterns, invariant tips, tool-usage strategies).
- Store reflections in an agent memory and retrieve relevant items for future tasks.
- Treat this as an **online-style** process because the agent immediately exploits per-episode feedback (success/failure) to adjust its prompt/memory before the next episode.

Behavior improves over time even though weights stay fixed; the agent‚Äôs ‚Äúpolicy‚Äù is shaped by reflection and memory.

#### 2.3 Paradigm 3 (Offline RL pipeline on top of SFT)

- **Behavior dataset construction:** collect full verification trajectories from VeriStruct, human experts, and the improved prompt-based agent. Each trajectory should contain state/action/observation tuples plus Verus outcomes.
- **Reward labeling:** assign dense rewards offline (e.g., +1 for verified module, +0.3 for VC reduction, ‚àí0.2 for syntax errors). Because this happens post hoc, we can apply sophisticated reward models or heuristics without affecting online throughput.
- **Offline RL algorithm:** apply conservative or batch RL objectives (e.g., Decision Transformer, CQL, IQL) to train a policy that respects the support of the dataset while improving toward the labeled reward. This can either fine-tune the base model directly or train a lightweight policy head that proposes edits/specs.
- **Bridging to online RL:** once offline policies are stable, we can safely switch to an online PPO/GRPO loop by seeding it with the offline-tuned policy and gradually enabling new data collection.

This offline-first pathway leverages the minimized Verus corpus plus saved agent traces, reducing risk compared to immediately launching an on-policy verifier loop.

### 3. Prompt-Only RL Plan for a Tool-Using VeriStruct-Style Agent

To ground Paradigm 2, we outline a concrete plan to improve VeriStruct via prompting-only RL. The core ideas are:

1. **Explicit tool-use interface:** expose Verus, compiler, and example libraries as callable tools (e.g., `run_verus`, `search_examples`, `simplify_vc`).
2. **Structured agent loop:** instead of a fixed pipeline, allow the agent to iteratively decide whether to refine invariants, inspect errors, call tools, or edit code.
3. **Reflection + memory:** after each episode, summarize lessons (e.g., ‚Äúring buffers need head/tail/len views,‚Äù ‚Äúadd lifetime bounds when encountering unconstrained lifetime errors‚Äù) and prepend them to future prompts via retrieval.

<details>
<summary><b>Click to expand: Detailed Agent Loop Implementation Sketch (~170 lines of Python)</b></summary>

The following Python skeleton illustrates how a tool-using VeriStruct agent could be organized. **This is a design document, not yet implemented.** It keeps the base LLM frozen but enables multi-step reasoning, tool calls, and reflection-driven improvements.

````python
from dataclasses import dataclass, field
from typing import Any, Dict, List, Literal, Optional

class Tool:
    name: str
    description: str

    def run(self, **kwargs) -> Dict[str, Any]:
        raise NotImplementedError

class VerusTool(Tool):
    name = "run_verus"
    description = "Run Verus on the current code and return success flag and error log."

    def __init__(self, verus_bin: str):
        self.verus_bin = verus_bin

    def run(self, file_path: str) -> Dict[str, Any]:
        success = False
        error_log = "VC failure at function foo: assertion may not hold"
        return {"success": success, "error_log": error_log}

class ExampleSearchTool(Tool):
    name = "search_examples"
    description = "Search verified examples/specs related to a keyword."

    def __init__(self, examples_index: Dict[str, str]):
        self.index = examples_index

    def run(self, query: str) -> Dict[str, Any]:
        hits = [code for k, code in self.index.items() if query.lower() in k.lower()]
        return {"matches": hits[:3]}

TOOLS: Dict[str, Tool] = {}


def register_tool(tool: Tool):
    TOOLS[tool.name] = tool


@dataclass
class AgentStep:
    thought: str
    action: Literal["CALL_TOOL", "EDIT_CODE", "FINISH"]
    tool_name: Optional[str] = None
    tool_args: Optional[Dict[str, Any]] = None
    edit_patch: Optional[str] = None
    observation: Optional[Dict[str, Any]] = None

@dataclass
class Episode:
    task_description: str
    initial_code: str
    steps: List[AgentStep] = field(default_factory=list)
    final_code: Optional[str] = None
    success: bool = False

REFLECTION_MEMORY: List[str] = []


def call_llm(system_prompt: str, messages: List[Dict[str, str]]) -> str:
    raise NotImplementedError


def parse_agent_action(raw_text: str) -> AgentStep:
    raise NotImplementedError


def run_verification_episode(task_description: str,
                             initial_code: str,
                             max_steps: int = 10) -> Episode:
    episode = Episode(task_description=task_description, initial_code=initial_code)
    current_code = initial_code
    system_prompt = build_system_prompt_with_reflections()

    messages: List[Dict[str, str]] = [
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": (
                "You are a Verus proof synthesis agent. Task:\n" + task_description + "\n\n"
                "Current code:\n```rust\n" + current_code + "\n```"
            ),
        },
    ]

    for step_idx in range(max_steps):
        raw_response = call_llm(system_prompt, messages)
        agent_step = parse_agent_action(raw_response)

        if agent_step.action == "CALL_TOOL":
            if agent_step.tool_name is None:
                raise ValueError("tool_name cannot be None for CALL_TOOL action")
            tool = TOOLS[agent_step.tool_name]
            obs = tool.run(**(agent_step.tool_args or {}))
            agent_step.observation = obs
            messages.append({"role": "assistant", "content": raw_response})
            messages.append({
                "role": "user",
                "content": f"Tool `{tool.name}` result:\n```json\n{obs}\n```"
            })

        elif agent_step.action == "EDIT_CODE":
            current_code = apply_patch(current_code, agent_step.edit_patch or "")
            episode.final_code = current_code
            messages.append({"role": "assistant", "content": raw_response})
            messages.append({
                "role": "user",
                "content": (
                    "Updated code is now:\n```rust\n" + current_code + "\n```"
                ),
            })

        elif agent_step.action == "FINISH":
            episode.final_code = current_code
            episode.steps.append(agent_step)
            break

        episode.steps.append(agent_step)

    verus_tool = TOOLS["run_verus"]
    result = verus_tool.run(file_path="CURRENT_MODULE.rs")
    episode.success = bool(result["success"])

    reflection = run_reflection(episode, result)
    if reflection:
        REFLECTION_MEMORY.append(reflection)

    return episode


def build_system_prompt_with_reflections(k: int = 5) -> str:
    recent_reflections = "\n\n".join(REFLECTION_MEMORY[-k:])
    return f"""
You are a Verus proof-synthesis and repair agent.
You have access to tools:
- run_verus(file_path): run the Verus verifier on the current code
- search_examples(query): retrieve similar verified examples
You must think step-by-step, decide when to call tools, and iteratively improve the code until it verifies.
Here are some lessons learned from previous episodes (if any):
{recent_reflections if recent_reflections else "(no reflections yet)"}
When you respond, ALWAYS output a JSON object describing your next action.
"""


def run_reflection(episode: Episode, final_result: Dict[str, Any]) -> Optional[str]:
    traj_summary = summarize_episode(episode, final_result)
    reflection_prompt = f"""
You are analyzing a proof-synthesis episode.
Episode summary:
{traj_summary}
Please extract 1-3 concrete, generalizable lessons that would help the agent do better on future, similar Verus verification tasks. Write them as bullet points.
"""
    try:
        reflection_text = call_llm(
            "You summarize lessons for a Verus verification agent.",
            [{"role": "user", "content": reflection_prompt}],
        )
        return reflection_text
    except Exception:
        return None


def apply_patch(code: str, patch: str) -> str:
    return code

def summarize_episode(ep: Episode, final_result: Dict[str, Any]) -> str:
    return f"Task: {ep.task_description}\nSuccess: {ep.success}\nSteps: {len(ep.steps)}"


def initialize_agent(verus_bin: str, examples_index: Dict[str, str]):
    register_tool(VerusTool(verus_bin))
    register_tool(ExampleSearchTool(examples_index))
````

</details>

**Summary:** This prompt-only RL approach enhances VeriStruct by (1) enabling deliberate tool-use, (2) supporting multi-step planning instead of a rigid pipeline, and (3) accumulating experience through reflections without modifying model weights. It can also serve as a scaffolding layer for future true-RL or hybrid SFT+RL experiments.

### 4. Prioritizing Offline RL Before Online RL

Given Verus‚Äôs relatively high evaluation cost, we plan to **start with offline RL** and use it to de-risk later online experiments. The same minimizer-powered corpus that feeds SFT also seeds the RL logs: for each minimized instance we first store the clean Q/A pair for SFT, then capture the richer reasoning trajectories (tool calls, intermediate hypotheses, Verus transcripts) that offline RL needs. This ensures we are "offline-first" by construction‚Äîevery RL policy sees only previously logged data until we explicitly green-light online exploration.

1. **Log every VeriStruct/Verus interaction now.** While running SFT evaluations or the prompt-based agent on minimizer-derived tasks, persist full transcripts (code snapshots, tool calls, Verus logs). This becomes the seed offline dataset that already aligns with the Q/A samples used for SFT.
2. **Label and filter trajectories offline.** Add success/failure flags, VC deltas, and metadata such as module size or error category. Remove degenerate runs to keep the distribution clean.
3. **Train offline policies.** Use Decision Transformers or conservative Q-learning variants to learn policies that predict next edits/specs conditioned on the logged context. Because training is offline, we can iterate rapidly without burning Verus cycles.
4. **Validate offline.** Roll out the learned policies in a simulator harness that replays logged states before attempting any fresh Verus calls. Ensure they do not overfit to logging artifacts.
5. **Gradually enable online updates.** Once offline evaluations look solid, transition to online PPO/GRPO or reflection-based loops that collect fresh data, using the offline policy as initialization so that exploration starts from a competent baseline.

This sequencing keeps compute costs manageable, respects Verus throughput limits, and provides a safer glide path toward full online RL.

---

### Phase 2: AST / Structured Representation Ablation (Bonus)

> **Status:** üìã Future work after text-only baselines are established

Once text-only SFT baselines are stable, we introduce **structured representations** as a scientific ablation to answer: *when does structure actually help?*

#### Candidate Structured Views

| Representation | Description | Use Case |
|----------------|-------------|----------|
| **Linearized AST** | Serialized Rust syntax tree | Complex nested structures |
| **Control-Flow Summary** | Loop structure + invariants | Loop-heavy proofs |
| **Verification State** | VC obligations + triggers | Understanding proof obligations |
| **Proof Graph** | Lemma dependencies | Higher-order reasoning |

#### Experimental Design

For each task (A, B, C), compare:

1. üìù **Text-only** (baseline)
2. üå≥ **Structure-only** (AST or other view)
3. üîÄ **Hybrid** (text + structure combined)

#### Expected Outcomes

Quantify *when* structure helps:
- ‚úÖ Long, nested loops with complex invariants
- ‚úÖ Tricky View functions and abstractions  
- ‚úÖ Higher-order lemmas and proof obligations
- ‚ùå Simple specifications and straightforward code

**Goal:** A clear empirical story like "Plain text achieves X% pass rate; AST adds +Y% only for loop-heavy tasks."

---

## Student Subprojects

This repo is designed to support multiple small research projects (e.g., rotation or undergraduate projects). Each subproject can be tackled independently:

| ID | Subproject | Status | Difficulty | Description |
|----|------------|--------|------------|-------------|
| 0 | **Rust/Verus Extraction & Minimization** | üìã Planned | Medium | Distinguish `exec`/`spec`/`proof` regions, prefer the Verus minimizer, and emit small dependency-free verifying snippets |
| 1 | **Dataset via Minimizer** | üìã Planned | Medium | Script minimizer calls, build JSONL datasets for Tasks A/B/C, implement deduplication |
| 2 | **SFT for Spec Generation (Task A)** | ‚úÖ Prototype | Easy | Train models on code ‚Üí spec, evaluate on held-out modules |
| 3 | **SFT for Verified Code Synthesis (Task B)** | üöß In Progress | Medium | Train models on spec ‚Üí code, evaluate by running Verus |
| 4 | **SFT for Proof/Invariant Repair (Task C)** | üìã Planned | Hard | Build dataset of (broken, error) ‚Üí (fixed), train repair models |
| 5 | **Benchmark & Evaluation Harness** | üìã Planned | Medium | Automate Verus compilation, execution, and metric collection |
| 6 | **AST/Structure Ablation Study** | üìã Planned | Advanced | Design AST encodings, run controlled ablations vs. text-only |
| 7 | **Qwen Model Baseline Evaluation** | üìã Planned | Easy-Medium | Run Qwen2.5-Coder models (0.5B, 1.5B, 7B) on Verus tasks, collect zero-shot and few-shot baselines, compare with GPT-2 prototype |

### Getting Started with a Subproject

1. **Choose a project** from the table above
2. **Review the current prototype** in `sft_example.py`
3. **Read the relevant methodology section** in this README
4. **Start small**: implement a minimal version, test it, then expand

#### Spotlight: Subproject 0 - Rust/Verus Extraction & Minimization

**Objectives**:

1. **Separate roles**: annotate or segment snippets so executable code, specifications, and proof/lemma blocks are clearly distinguishable for downstream datasets.
2. **Emit tiny verifying units**: favor small, clean, compilable/verifiable samples with zero external dependencies. When possible, run the Verus minimizer first to shrink source programs before manual extraction.

**Where to start**:

- Follow the [Rust/Verus Code Extraction Method](./RUST_VERUS_EXTRACTION.md) for a step-by-step process.
- Use the [`verus_code_extraction.py`](./verus_code_extraction.py) scaffold to automate discovery and verification, and log manifest entries for accepted snippets.
- Keep provenance (paths, git SHAs, verifier output) so samples can be regenerated or audited.

**Deliverables**:

- A repeatable script or notebook that walks a target repo, prefers minimizer outputs when available, and writes JSONL manifests of dependency-free verifying snippets, tagged by `exec`/`spec`/`proof` content.
- Example snippets demonstrating the separation of code, specs, and proofs for SFT/RL tasks.

#### Spotlight: Subproject 7 - Qwen Model Baseline Evaluation

**Goal**: Establish strong baseline results for code-specialized models before fine-tuning.

**Why this matters**: The current prototype uses GPT-2 (a general-purpose model). We need to measure how well code-specialized models like Qwen2.5-Coder perform on Verus tasks *before* fine-tuning to quantify the value of SFT.

**Deliverables**:
1. **Inference script** for running Qwen models on Verus prompts
2. **Zero-shot evaluation**: Raw model performance on 10-20 test examples
3. **Few-shot evaluation**: Performance with 3-5 in-context examples
4. **Comparison report**: Baseline metrics vs. GPT-2 (see [Evaluation](#phase-1-evaluation))
5. **Documentation**: Best prompts, temperature settings, and failure modes

**Technical steps**:
```python
# 1. Load Qwen model
from transformers import AutoModelForCausalLM, AutoTokenizer
model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen2.5-Coder-1.5B")
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen2.5-Coder-1.5B")

# 2. Create test prompts from existing examples
test_prompts = [...]  # From build_dataset() in sft_example.py

# 3. Generate completions
# 4. Parse outputs and run Verus verification
# 5. Compute pass rates and error breakdowns
```

**Estimated effort**: 2-3 weeks
- Week 1: Setup, inference script, zero-shot results
- Week 2: Few-shot prompting experiments
- Week 3: Verus verification, analysis, report

**Prerequisites**: 
- GPU with 8GB+ VRAM (for 1.5B model) or CPU (slower)
- Verus installation (see [Installation](#installation))
- Familiarity with Hugging Face transformers

**Bonus extensions**:
- Compare multiple model sizes (0.5B, 1.5B, 7B)
- Test different prompting strategies (chain-of-thought, etc.)
- Analyze which error types are most common
- Create a leaderboard for tracking improvements

---

## Installation

### Prerequisites
- Python 3.8 or higher
- PyTorch 2.0+
- CUDA-capable GPU (recommended, but CPU training is possible)
- Verus verifier (required for evaluation and dataset generation)

### Setup

#### 1. Install Python Dependencies

```bash
# Clone the repository
git clone https://github.com/ChuyueSun/VerusSFT.git
cd VerusSFT

# Install dependencies
pip install transformers trl datasets peft accelerate torch

# Optional: Install with specific PyTorch version for your CUDA
# pip install torch --index-url https://download.pytorch.org/whl/cu118
```

#### 2. Install Verus (Required for Full Functionality)

```bash
# Clone Verus repository
git clone https://github.com/verus-lang/verus.git
cd verus

# Download Z3 SMT solver
./tools/get-z3.sh

# Activate Verus environment
source tools/activate

# Build Verus (takes several minutes)
vargo build --release

# Add Verus to PATH (add to ~/.bashrc or ~/.zshrc for persistence)
export PATH=$PATH:$(pwd)/source/target-verus/release
```

#### 3. Verify Installation

```bash
# Verify Python dependencies
python -c "import transformers, trl, peft; print('All dependencies installed successfully!')"

# Verify Verus installation
verus --version
```

> **Note**: The current prototype (SFT training only) does not require Verus. However, Verus is essential for:
> - Evaluating generated code via verification
> - Using the minimizer for dataset generation
> - Running the full research pipeline

## Usage

### 1. Training

Run the training script to fine-tune the model on Verus examples:

```bash
python sft_example.py
```

**What happens during training:**
- Loads GPT-2 base model and tokenizer
- Applies LoRA adapters for efficient fine-tuning
- Trains on 10 Verus examples for 10 epochs
- Saves adapter weights to `./sft_output/` (~6MB)

**Expected output:**
```
Loading model...
Training...
Epoch 1/10: loss=3.40
...
Epoch 10/10: loss=2.42
Training complete! Model saved to ./sft_output/
```

**Training time:** ~15 seconds on a modern GPU, ~2 minutes on CPU

### 2. Inference

Test the trained model with new prompts:

```bash
python test_inference.py
```

**What happens:**
- Loads base model + trained LoRA adapter
- Runs 3 test prompts:
  1. Adding specs to a clamp function
  2. Writing a multiply function
  3. Adding specs to a min function
- Generates Verus code completions

**Example output:**
```
Test 1: Add Verus specs to this function:
Generated Code:

fn clamp(x: i32, min: i32, max: i32) -> i32 {
    requires min <= max;
    ensures result >= min && result <= max;
    ...
}
```

### 3. Customizing Training

Edit `sft_example.py` to:
- **Add more examples**: Modify `build_dataset()` function
- **Change model**: Update `model_name` variable
- **Adjust hyperparameters**: Modify the `SFTConfig` settings
- **Change LoRA config**: Update `LoraConfig` parameters

---

## Configuration

### Training Parameters (in `sft_example.py`)

| Parameter | Default | Description | Tuning Tips |
|-----------|---------|-------------|-------------|
| `num_train_epochs` | 10 | Number of training epochs | Increase for larger datasets |
| `per_device_train_batch_size` | 2 | Batch size per device | Reduce if OOM errors occur |
| `learning_rate` | 3e-4 | Learning rate | Lower for stability, higher for faster convergence |
| `max_seq_length` | 1024 | Maximum sequence length | Must fit prompt + completion |
| `gradient_accumulation_steps` | 2 | Steps to accumulate gradients | Increase for larger effective batch size |

### LoRA Configuration

| Parameter | Default | Description | Notes |
|-----------|---------|-------------|-------|
| `r` | 16 | LoRA rank | Higher = more capacity but larger adapter |
| `lora_alpha` | 32 | LoRA scaling parameter | Typically 2√ó the rank |
| `target_modules` | `["c_attn", "c_proj"]` | Layers to apply LoRA | GPT-2 specific; adjust for other models |
| `lora_dropout` | 0.05 | Dropout rate | Prevents overfitting |

### Switching Base Models

The current implementation uses GPT-2. To use a different model, edit `sft_example.py`:

```python
# Replace this line:
model_name = "gpt2"

# With one of these recommended models:
model_name = "Qwen/Qwen2.5-Coder-1.5B"  # Recommended for code
# model_name = "bigcode/starcoder2-3b"
# model_name = "deepseek-ai/deepseek-coder-1.3b-base"
```

**Note**: When changing models, update `target_modules` in the LoRA config to match the new model's architecture.

---

## Compute Requirements

### Hardware Recommendations

| Use Case | RAM | GPU | Training Time (10 epochs) | Notes |
|----------|-----|-----|---------------------------|-------|
| **Prototype (GPT-2)** | 8GB | CPU or any GPU | ~2 min (CPU), ~15s (GPU) | Good for testing pipeline |
| **Small models (1.5B)** | 16GB | 8GB VRAM (RTX 3070+) | ~5-10 min | Qwen2.5-Coder-1.5B, DeepSeek-Coder-1.3B |
| **Medium models (3B)** | 32GB | 16GB VRAM (RTX 4080+) | ~15-30 min | StarCoder2-3B, CodeLlama-7B with aggressive LoRA |
| **Large models (7B+)** | 64GB+ | 24GB+ VRAM (RTX 4090/A100) | 1-3 hours | Full-scale research experiments |

### Disk Space

- **Python dependencies**: ~5GB (PyTorch, transformers, etc.)
- **Base model cache**: 0.5GB (GPT-2) to 15GB (7B models)
- **LoRA adapters**: 6MB (r=16) to 50MB (r=64)
- **Verus**: ~2GB (includes Z3 and dependencies)
- **Dataset**: 10MB (current) to 500MB+ (planned full dataset)

**Recommended**: 20GB free disk space for comfortable development

### Cloud Computing Options

For users without local GPU access:

- **Google Colab** (Free tier): Sufficient for GPT-2 and small models, 12GB GPU
- **Google Colab Pro**: Recommended for 1.5B-3B models, better GPU options
- **AWS/GCP/Azure**: For large-scale experiments and production training
- **Lambda Labs / RunPod**: Cost-effective GPU rentals for research

### Performance Tips

1. **Reduce memory usage**:
   - Lower `per_device_train_batch_size` (1 instead of 2)
   - Reduce `max_seq_length` (512 instead of 1024)
   - Use smaller LoRA rank (`r=8` instead of `r=16`)
   - Enable gradient checkpointing

2. **Speed up training**:
   - Use mixed precision training (`bf16=True` for modern GPUs)
   - Enable gradient accumulation for effective larger batches
   - Use flash attention if available

3. **Dataset generation** (Verus minimizer):
   - CPU-bound, not GPU-intensive
   - Can run on modest hardware (8GB RAM, any CPU)
   - Parallelizable across multiple examples

---

## Dataset

### Current Dataset (Seed Set)

The training dataset includes **10 examples** covering common Verus patterns:

| Task Type | Examples | Coverage |
|-----------|----------|----------|
| Specification Generation | 5 | `abs`, `max`, `min`, array bounds, division |
| Code Synthesis | 3 | `add_one`, `double`, `subtract` |
| Combined Spec + Code | 2 | `is_positive`, `square` |

**Example format:**
```python
{
    "text": "Add Verus specs to this function:\n```rust\n...\n```\n```verus\n...\n```"
}
```

### Adding Your Own Examples

Edit the `build_dataset()` function in `sft_example.py`:

```python
examples = [
    {
        "text": "Your task description\n```verus\nYour Verus code here\n```"
    },
    # Add more examples...
]
```

### Additional Verus Code Sources

Expand your dataset with these verified Verus projects:

| Source | Type | Link |
|--------|------|------|
| **Verus Examples** | Core examples & tests | [verus-lang/verus](https://github.com/verus-lang/verus/tree/main/source/rust_verify_test/tests) |
| **VOSTD** | Auto-generated specs | [asterinas/vostd](https://github.com/asterinas/vostd) |
| **Verismo** | Verified systems | [microsoft/verismo](https://github.com/microsoft/verismo) |
| **Memory Allocator** | Memory safety | [verus-lang/verified-memory-allocator](https://github.com/verus-lang/verified-memory-allocator) |
| **Verified Storage** | Storage systems | [microsoft/verified-storage](https://github.com/microsoft/verified-storage) |
| **Vericoding** | Benchmarks | [Beneficial-AI-Foundation/vericoding](https://github.com/Beneficial-AI-Foundation/vericoding) |

---

## File Structure

```
VerusSFT/
‚îú‚îÄ‚îÄ sft_example.py          # Main training script with dataset builder
‚îú‚îÄ‚îÄ test_inference.py       # Inference/testing script
‚îú‚îÄ‚îÄ README.md               # This file: documentation + research proposal
‚îú‚îÄ‚îÄ .gitignore              # Git ignore rules (excludes sft_output/)
‚îî‚îÄ‚îÄ sft_output/             # Generated after training (not tracked by git)
    ‚îú‚îÄ‚îÄ adapter_model.safetensors  # LoRA adapter weights (~6MB)
    ‚îú‚îÄ‚îÄ adapter_config.json         # LoRA configuration
    ‚îú‚îÄ‚îÄ tokenizer_config.json       # Tokenizer settings
    ‚îî‚îÄ‚îÄ [other tokenizer files]     # Vocabulary, merges, etc.
```

### What Gets Tracked

- ‚úÖ Source code (`*.py`)
- ‚úÖ Documentation (`*.md`)
- ‚úÖ Configuration files
- ‚ùå Model outputs (`sft_output/`)
- ‚ùå Python cache (`__pycache__/`)
- ‚ùå Jupyter checkpoints

## FAQ

<details>
<summary><b>Why start with GPT-2 instead of a larger code model?</b></summary>

GPT-2 is used for the initial prototype because it:
- Trains quickly (~15 seconds) for rapid iteration
- Works on CPU or small GPUs
- Validates the training pipeline before scaling up

**Next step:** Switch to code-specific models like Qwen2.5-Coder or StarCoder2.
</details>

<details>
<summary><b>How is this different from general code fine-tuning?</b></summary>

VerusFT-RL focuses specifically on:
- **Verification-aware training**: Not just code syntax, but specs and proofs
- **Verus-native evaluation**: Success = code that actually verifies
- **Minimized examples**: Small, focused training samples vs. large codebases
- **Multi-task learning**: Spec generation, code synthesis, and error repair
</details>

<details>
<summary><b>Can I use this for languages other than Rust/Verus?</b></summary>

The approach is general and could work for:
- Dafny
- F*
- Coq
- Lean

Just replace the dataset and adjust the prompts. The SFT pipeline remains the same.
</details>

<details>
<summary><b>Do I need a GPU?</b></summary>

- **For prototyping**: CPU works fine (~2 min training time)
- **For larger models**: GPU recommended (Qwen2.5-Coder-1.5B needs ~6GB VRAM)
- **For production**: Multi-GPU setup for models like StarCoder2-3B
</details>

<details>
<summary><b>How do I add my own Verus examples?</b></summary>

Edit `build_dataset()` in `sft_example.py`:

```python
examples = [
    {"text": "Your task prompt\n```verus\nYour code\n```"},
    # Add more...
]
```

See the [Dataset](#dataset) section for more details.
</details>

---

## Troubleshooting

### Common Issues

#### CUDA Out of Memory
```bash
# Reduce batch size in sft_example.py
per_device_train_batch_size=1  # instead of 2

# Or reduce sequence length
max_seq_length=512  # instead of 1024

# Or reduce LoRA rank
r=8  # instead of 16
```

#### Model Loading Errors
```bash
# If you get tokenizer errors, ensure pad token is set
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# Clear cache and retry
rm -rf ./sft_output
python sft_example.py
```

#### Import Errors
```bash
# Ensure all dependencies are installed
pip install --upgrade transformers trl datasets peft accelerate torch

# Check versions
pip show transformers trl peft
```

#### Training Too Slow
- Enable gradient checkpointing: `gradient_checkpointing=True` in SFTConfig
- Use mixed precision: `fp16=True` (for older GPUs) or `bf16=True` (for newer GPUs)
- Reduce `max_seq_length` if examples are shorter

### Getting Help

- **Verus Documentation**: [https://verus-lang.github.io/verus/](https://verus-lang.github.io/verus/)
- **TRL Documentation**: [https://huggingface.co/docs/trl](https://huggingface.co/docs/trl)
- **PEFT Documentation**: [https://huggingface.co/docs/peft](https://huggingface.co/docs/peft)

---

## Future Improvements

1. **Expand dataset**: Add 50‚Äì100+ diverse Verus examples using the minimizer
2. **Better base model**: Evaluate code-specific models (see [Configuration](#configuration))
3. **Evaluation**: Add metrics for Verus specification correctness and verification pass rate
4. **Fine-tune generation**: Adjust decoding parameters (temperature, top-p, beam search)
5. **Structured ablations**: Quantify when AST or other structured signals improve verification outcomes
6. **Integration**: Connect to Verus minimizer for automated dataset generation

---

## License

This project is licensed under the MIT License. See LICENSE file for details.

## Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## Citation

If you use this work in your research, please cite:

```bibtex
@software{verusft2025,
  title={VerusSFT: Fine-Tuning and RL for Verification-Oriented Rust/Verus Code},
  author={Sun, Chuyue},
  year={2025},
  url={https://github.com/ChuyueSun/VerusSFT},
  note={Research prototype for LLM-assisted formal verification}
}
```

## Acknowledgments

- **[Verus](https://github.com/verus-lang/verus)** - The Verus verification system for Rust
- **[Hugging Face](https://huggingface.co/)** - Transformers library and model hub
- **[PEFT](https://github.com/huggingface/peft)** - Parameter-efficient fine-tuning library
- **[TRL](https://github.com/huggingface/trl)** - Transformer Reinforcement Learning library

## Contact

For questions or collaboration opportunities, please open an issue or reach out to the maintainers.
